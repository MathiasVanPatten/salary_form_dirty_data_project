{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pycountry\n",
    "# %pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import pycountry\n",
    "from fuzzywuzzy import process\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the dataset and what I'll be keeping\n",
    "Hi there, this project is mostly a tableau focused one but this script will explain how I processed, cleaned, and repackaged the originally dirty data to a nicer form that tableau can easily use. The data itself comes from a websurvey mostly answered at the end of April 2021, accessible [here](https://www.askamanager.org/2021/04/how-much-money-do-you-make-4.html). I'll walk through the columns, briefly explain what the data in them looks like, and explain my plan for them going forward. \n",
    "\n",
    "There are 28k entries in this csv with 18 features:\n",
    "1. Timestamp: the data and time of the survey response. Won't use, most of the responses are within a few days of each other so not very interesting\n",
    "2. How old are you?: ordinal but stored as the value of response (i.e. 25-34), will use after conversion back to ordinal\n",
    "3. What industry do you work in?: ordinal again but stored as the value of the reponse. Will convert back to ordinal and use. For now I will toss out Other\n",
    "4. Job Title: free form string, will search for some keywords to see if I can get some measure of seniority from it\n",
    "5. Job Title Context: free form string, won't use, people used this to give additional context to their job title and industry but it's often not responded to and varies widely in response quality\n",
    "6. Annual salary: free form string but generally people seemed to do it right, some gave it in units of kUSD so I'll convert that back to USD. Any non USD currencies I'll be using the exchange rate from the eurofxref.csv file in data to convert it to USD.\n",
    "7. Additional Salary: same as the annual salary, will add them together eventually\n",
    "8. Currency: string that expresses the type of currency salary expressed in\n",
    "9. Currency Context: allows for expansion on the other response to currency, will ignore these to keep things simple\n",
    "10. Additional Compensation context: won't use, few responses and highly variable\n",
    "11. Country: free form string, will have to normalize after grouping the common terms for the same country (i.e. USA, US, usa, United States, etc.)\n",
    "12. State: ordinal choice, the 50 states of the United States\n",
    "13. What city do you work in: free form string but generally people answered it well. For this one I'm thinking of using the unique set and using that to determine which ones to keep. Big issues around mispellings and alternative expressions in this one (i.e. LA and Los Angeles)\n",
    "14. Years of professional exp: ordinal but stored as the value of the response again, will use after conversion\n",
    "15. Years of professional exp in the claimed industry: ordinal but stored as the value of the response again, will use after conversion\n",
    "16. Highest level of education completed: same ordinal as before, will use\n",
    "17. Gender: same ordinal as before, will use\n",
    "18. Race: this allowed for multiple responses, I think I'll keep the non white one if they answered white and another otherwise I'll just take the first one. I'll try to see if tableau could handle multiple responses well\n",
    "\n",
    "I'll rename the columns to something shorter since right now they're all the questions of the fields they represent and are quite long then drop all the ones I won't be using at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/form_rep.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_axis(['Datetime', \n",
    "'Age', \n",
    "'Industry', \n",
    "'Job Title', \n",
    "'Job Title Context', \n",
    "'Salary', \n",
    "'Salary Additional', \n",
    "'Currency', \n",
    "'Currency Context', \n",
    "'Salary Context', \n",
    "'Country', \n",
    "'State', \n",
    "'City', \n",
    "'Years Total', \n",
    "'Years Specific', \n",
    "'Education', \n",
    "'Gender', \n",
    "'Race'], axis = 1)\n",
    "drop_list = ['Datetime', 'Job Title Context', 'Salary Context', 'Currency Context']\n",
    "df.drop(columns = drop_list, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing I want to do is do the salary conversion. I'm using the exhange rates I got from the european bank database today 6/10/2024 and will use that to convert the salaries to USD. I'll be dropping the 120 entries that use other to keep things simple and changing the AUD/NZD entry to just NZD. Then I'll use the conversion function to change the salaries to USD after changing any entries that are three digits or less (assuming that's supposed to be kCurrency intead of Currency) to the proper units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['Currency'] == 'Other'].index, inplace=True)\n",
    "df.loc[df['Currency'] == 'AUD/NZD', 'Currency'] = 'NZD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sals(x):\n",
    "    if len(x)<=3:\n",
    "        return int(x.replace(',', ''))*1000 #likely inputted in kUnits of money\n",
    "    else:\n",
    "        return int(x.replace(',', ''))\n",
    "    \n",
    "df['Salary'] = df['Salary'].apply(convert_sals) #salary needs conversion, salary additional is already a float. Can't assume units for it either\n",
    "df.loc[df['Salary Additional'].isna(), 'Salary Additional'] = 0 #filling in the NaNs with 0s\n",
    "df['Salary'] = df['Salary'] + df['Salary Additional']\n",
    "df.drop(columns=['Salary Additional'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conv_df():\n",
    "    conv_df = pd.read_csv('data/eurofxref.csv').rename(columns=lambda x: x.strip()) #some of the currencies end up have leading whitespaces otherwise\n",
    "    conv_df.insert(0, 'EUR', 1)\n",
    "    return conv_df\n",
    "\n",
    "def convert_to_usd(amount, cur_str, conv_df):\n",
    "    return int((amount * conv_df['USD'].values / conv_df[cur_str].values)[0])\n",
    "\n",
    "conv_df = load_conv_df()\n",
    "df['Salary'] = df.apply(lambda row: convert_to_usd(row['Salary'], row['Currency'], conv_df), axis=1)\n",
    "df['Currency'] = 'USD' #we've done the conversion so now all are in USD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I'm going to tackle the industry and job title sections. First for industry as I said I'll be dropping any of the fill in your own ones since the choices were already expansive. We lose about 1000 entries for doing this, we're now at 25800 just about. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_industry_list = ['Accounting, Banking & Finance',\n",
    "'Agriculture or Forestry',\n",
    "'Art & Design',\n",
    "'Business or Consulting',\n",
    "'Computing or Tech',\n",
    "'Education (Primary/Secondary)',\n",
    "'Education (Higher Education)',\n",
    "'Engineering or Manufacturing',\n",
    "'Entertainment',\n",
    "'Government and Public Administration',\n",
    "'Health care',\n",
    "'Hospitality & Events',\n",
    "'Insurance',\n",
    "'Law',\n",
    "'Law Enforcement & Security',\n",
    "'Leisure, Sport & Tourism',\n",
    "'Marketing, Advertising & PR',\n",
    "'Media & Digital',\n",
    "'Nonprofits',\n",
    "'Property or Construction',\n",
    "'Recruitment or HR',\n",
    "'Retail',\n",
    "'Sales',\n",
    "'Social Work',\n",
    "'Transport or Logistics',\n",
    "'Utilities & Telecommunications']\n",
    "df = df[df['Industry'].isin(valid_industry_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the senority terms I've defined a few levels using common terms I found scrolling around on the dataframe. It's not exhaustive but it should be alright. The default will be the mid level. After running it we get about 14k entries in the mid level with the rest distributed in the other levels. Pretty good for a quick estimate of seniority. Note that since we go down the list if someone has multiple titles that match we'll end up assigning them to the highest level (i.e. there's one entry that is Associate Senior Director and that goes to Executive Level as I think it should). Obviously the difference in job titles between industries is an issue here and if it were a bigger project it'd be worth really breaking it down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mid level will be the default so left off\n",
    "senority_terms = { 'Junior Level': ['associate', 'jr', 'jr.', 'junior', 'intern'],\n",
    "                    'Senior Level' : ['senior', 'sr', 'sr.'],\n",
    "                    'Managerial Level' : ['lead', 'manager', 'head'],\n",
    "                    'Executive Level' : ['president', 'ceo', 'executive', 'director']}\n",
    "df['Senority'] = 'Mid Level'\n",
    "for senior_label, senior_list in senority_terms.items():\n",
    "    df.loc[df['Job Title'].str.contains('|'.join(senior_list), case=False), 'Senority'] = senior_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, moving on to some demographical stuff. Education and Race.\n",
    "\n",
    "Education first, there are 200 missing entries in it. The options didn't allow for people who didn't finish high school to answer that but looking at the job titles of the people who didn't answer education I can safely conclude they did finish high school so I'll drop those 200 (some of the titles included Attorneys, Professors, and Directors of Hospitals, job titles that need a high education, these are the majority of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['Education'].isna()].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for race, 122 entries didn't answer so those will be dropped. The rest need to be simplified down to a single race to make filtering in tableau easier. The most common multi-race choice was something and white. We can just take the something since white dominates the set and we're interested in the contrast for this feature. There are a small handful of hybrid choices that don't include white. For that I'll just take the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['Race'].isna()].index, inplace=True)\n",
    "\n",
    "non_white_race_options = ['Asian or Asian American',\n",
    "                'Black or African American',\n",
    "                'Hispanic, Latino, or Spanish origin',\n",
    "                'Middle Eastern or Northern African',\n",
    "                'Native American or Alaska Native',\n",
    "                'Another option not listed here or prefer not to answer']\n",
    "def simplify_race(race_str, non_white_race_options):\n",
    "    for nwr_option in non_white_race_options:\n",
    "        if nwr_option in race_str:\n",
    "            return nwr_option\n",
    "    return 'White'\n",
    "\n",
    "df['Race'] = df['Race'].apply(lambda x: simplify_race(x, non_white_race_options))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the hardest part, the location. I'm going to keep the state as is for now and see how tableau handles the nans. State was chosen from a checklist so that's easy. For country though, I'm going to use a country list from pycountry which compiles the ISO country list and I'll use fuzzy string matching to try and get past typos. I ran this a few times and compiled the most common mistakes neither system handled right into the manual fixes to try and get as much good data through as I could. Some where unrecoverable due to respondants adding too much detail but that lost only a few hundred results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "doge = pycountry.countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_names = [country.name for country in pycountry.countries]\n",
    "country_a3_names = [pycountry.countries.get(name = country).alpha_3 for country in country_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_country(country_str, country_dict_list, manual_fixes):\n",
    "    try:\n",
    "        closest_match = process.extractOne(country_str, list(manual_fixes.keys()), score_cutoff=85)\n",
    "        if closest_match:\n",
    "            status_message = f'Input: {country_str} found a match on the manual override dicts: {manual_fixes[closest_match[0]]}'\n",
    "            # print(status_message)\n",
    "            return manual_fixes[closest_match[0]], status_message, country_str\n",
    "        country_match = pycountry.countries.search_fuzzy(country_str)[0] #searches within subdivisions of countries as well so it's a good first check\n",
    "        status_message = f'Input: {country_str} found a match on the pyc fuzzy search: {country_match.name}'\n",
    "        # print(status_message)\n",
    "        return country_match.name, status_message, country_str\n",
    "    except:\n",
    "        for country_dict in country_dict_list:\n",
    "            closest_match = process.extractOne(country_str, list(country_dict.keys()), score_cutoff=85)\n",
    "            if closest_match:\n",
    "                status_message = f'Input: {country_str} found a match on the dicts: {country_dict[closest_match[0]]}'\n",
    "                # print(status_message)\n",
    "                return country_dict[closest_match[0]], status_message, country_str\n",
    "        status_message = f'Input: {country_str} found no matches'\n",
    "        # print(status_message)\n",
    "        return 'ERROR', status_message, country_str #if we couldn't find any match we nan out the country for removal\n",
    "\n",
    "# df['Country'] = df['Country'].apply(lambda x: simplfy_country(x))\n",
    "manual_fixes_keys = ['US', 'ISA', 'UK', 'New Zealand', 'Australia', 'NA', 'Vietnam', 'Denmark', 'United States of America', 'America']\n",
    "manual_fixes_values = ['United States', 'United States', 'United Kingdom', 'New Zealand', 'Australia', 'United States', 'Vietnam', 'Denmark', 'United States', 'United States']\n",
    "\n",
    "manual_fixes = dict(zip(manual_fixes_keys, manual_fixes_values))\n",
    "country_a3_2_name = {a3_name : countryn for countryn, a3_name in zip(country_names, country_a3_names)}\n",
    "country_name_2_name = {countryn : countryn for countryn in country_names}\n",
    "\n",
    "\n",
    "country_dicts = [country_name_2_name, country_a3_2_name]\n",
    "\n",
    "# for cd in country_dicts:\n",
    "df['Country'] = df['Country'].apply(lambda x: \"\".join(x for x in x if (x in string.ascii_letters) or (x==' '))) #cleaning any non letters/spaces out first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask.dataframe as dd\n",
    "\n",
    "# # Convert the pandas DataFrame to a Dask DataFrame\n",
    "# ddf = dd.from_pandas(df, npartitions=8)\n",
    "\n",
    "# # Define a Dask delayed function for simplify_country\n",
    "# from dask import delayed\n",
    "\n",
    "# @delayed\n",
    "# def delayed_simplify_country(country_str, country_dict_list):\n",
    "#     return simplify_country(country_str, country_dict_list)\n",
    "\n",
    "# # Apply the delayed function to the Dask DataFrame\n",
    "# ddf['Country'] = ddf['Country'].apply(lambda x: delayed_simplify_country(x, country_dicts), meta=('x', 'object'))\n",
    "\n",
    "# # Compute the Dask DataFrame to get the result\n",
    "# df = ddf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doge = df['Country'].values\n",
    "# doge2 = doge[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doge2.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1718078181.3550684\n",
      "20274/25335, ETA: 0.18 min"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: ' ']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21746/25335, ETA: 0.13 min"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: ' ']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24079/25335, ETA: 0.04 min"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: ' ']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25334/25335, ETA: 0.00 min"
     ]
    }
   ],
   "source": [
    "test_cases = []\n",
    "total_in = len(df['Country'].values)\n",
    "st = time.time()\n",
    "print(st)\n",
    "for xind, x in enumerate(df['Country'].values):\n",
    "    mean_rate = (time.time()-st)/(xind+1)\n",
    "    print(f'\\r{xind}/{total_in}, ETA: {mean_rate*(total_in-xind)/60:.2f} min', end='', flush=True)\n",
    "    test_cases.append(simplify_country(x, country_dicts, manual_fixes))\n",
    "# test_cases = [simplify_country(x, country_dicts) for x in df['Country'].values]\n",
    "# df['Country'] = df['Country'].apply(lambda x: simplify_country(x, country_dicts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doge = np.array(test_cases)\n",
    "# failed_cases = []\n",
    "# for tc in test_cases:\n",
    "#     if tc == np.nan:\n",
    "#         failed_cases.append(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm_stack=[]\n",
    "# cs = 'United kindom'\n",
    "# for country_dict in country_dicts:\n",
    "#     closest_match = process.extractOne(cs, list(country_dict.keys()), score_cutoff=80)\n",
    "#     if closest_match:\n",
    "#         cm_stack.append((cs, country_dict[closest_match[0]]))\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_cases= [x for x in test_cases if x[0] == 'ERROR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "doge = np.array(test_cases)\n",
    "# unique_count = set(doge[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to drop city since it won't add enough to be worth the trouble of ironing out the quality of that field. The countries took a good bit enough. I'll also drop the job title since the information from them are encoded into senority and the industry. At least for a high level look.\n",
    "\n",
    "With that, we have the dataframe ready to go for tableau so we drop a save and head over to the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['Job Title', 'City'], inplace=True)\n",
    "df.to_csv('data/cleaned_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
